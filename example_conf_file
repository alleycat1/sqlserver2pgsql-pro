#
# This file is an example of a sqlserver2pgsql configuration file.
#

#
# Files location.
#

# Input file.
sql server dump filename = /tmp/dump          # the source SQL-Server Dump (obviously compulsory)

# Output files.
before file              = /tmp/before        # SQL script to execute before loading the data
after file               = /tmp/after         # SQL script to execute after having loaded the data
unsure file              = /tmp/unsure        # SQL script containing statements to check or to adjust manually
kettle directory         = /tmp/kettle        # comment this line if you don't want kettle components to be generated

#
# Optional parameters to setup specific behaviour.
#

#case insensitive         = 0     # set it to 1 to generate a dump with citext and check constraints all over the place
#skip citext length check = 0     # set it to 1 to not add a CHECK (char_length()) constraint for citext fields (when case insensitive is set to 1)
#no relabel dbo           = 1     # set it to 0 to convert the dbo schema into public
#relabel schemas          = dbo=>foo;schema1=>bar
#convert numeric to int   = 1     # set it to 0 to keep numeric(xx,0) types as numeric(xx,0); they will be converted to smallint, int or bigint by default
#keep identifier case     = 0     # set it to 1 to keep the case of database objects; by default identifier names are converted to lowercase
#camelcasetosnake         = 0     # set it to 1 to convert identifiers from camelCase to snake_case; keep identifier case and camelcasetosnake cannot be both set to 1
#col map file             = /tmp/map           # text file with SQL-Server and PostgreSQL schemas, tables and columns names
#col map file header      = 0     # set if to 1 to add a header line to the col map file
#col map file delimiter   = \t    # the fields delimiter in the col map file (TAB by default)
#validate constraints     = yes   # should the constraints be validated by the DDL scripts ? 'yes' = validated at constraint creation,
                                  #   'no' = kept NOT VALID, 'after' = validated after the data load; 'yes' by default
#use identity column      = 1     # set it to 0 to create an explicite SEQUENCE for identity columns (the old technic); by default,
                                  #   GENERATED ALWAYS clauses are generated
#drop rowversion          = 0     # set it to 1 to ignore columns of SQL-Server type 'rowversion' or 'timestamp'; 0 by default

#
# Parameters used for the data migration with Kettle.
# They are ignored as long as the 'kettle directory' parameter is not set.
#

sql server database      = foo
sql server host          = foo_host
sql server host instance = my_instance        # optional when the default instance is used
sql server port          = 1433
sql server username      = foo_user
sql server password      = foo_password

postgresql database      = bar
postgresql host          = bar_host
postgresql port          = 5432
postgresql username      = bar_user
postgresql password      = bar_password

#postgresql force ssl     = 0     # set it to 1 to force a SSL session to PostgreSQL; 0 by default

#parallelism_in           = 1     # parallelism degree when reading from SQL-Server (where available); 1 by default
#parallelism_out          = 8     # number of parallel connections used by kettle to insert data into the PostgreSQL database; 8 by default

#stringtype unspecified   = 0     # set it to 1 to let kettle process textual data as "not necessarily a strict PostgreSQL VARCHAR data"; 0 by default
#ignore errors            = 0     # set it to 1 to not abort the data migration job when an error occurs; the parameter is ignored for incremental jobs
                                  #   warning: the migration will be slower and the job's log will need to be throroughly examined
# Incremental job parameters.
#sort size                = 10000 # drives the amount of memory and temporary files that will be created by an incremental job; 10000 by default
#use pk if possible       = 0     # set to either 1 or a space separated list of schema qualified table names, for tables candidated for getting already sorted rows
